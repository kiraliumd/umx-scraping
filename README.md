# UMX Scraper (Livelo)

Este √© um microservi√ßo de automa√ß√£o especializado em extra√ß√£o de saldo do programa **Livelo**, integrado com **AdsPower** para gest√£o de impress√µes digitais (fingerprints) e **Supabase** para persist√™ncia de dados.

## üöÄ Como Rodar no Windows

### 1. Pr√©-requisitos
*   **Python 3.10+**: Certifique-se de que o Python est√° instalado e adicionado ao seu PATH.
*   **AdsPower**: O software AdsPower deve estar aberto e logado.
*   **Google Sheets API**: Se estiver usando o logger de planilhas, certifique-se de ter o arquivo `service_account.json`.

### 2. Configura√ß√£o do Ambiente
1. Clone o reposit√≥rio ou baixe os arquivos.
2. Abra o terminal (PowerShell ou CMD) na pasta do projeto.
3. Crie e ative um ambiente virtual:
   ```powershell
   python -m venv venv
   .\venv\Scripts\activate
   ```
4. Instale as depend√™ncias:
   ```powershell
   pip install -r requirements.txt
   ```

### 3. Configura√ß√£o das Vari√°veis (.env)
Crie um arquivo `.env` na raiz do projeto com o seguinte modelo:
```env
SUPABASE_URL=https://sua-url.supabase.co
SUPABASE_KEY=sua-chave-aqui
ADSPOWER_API_URL=http://127.0.0.1:50325
ADSPOWER_API_KEY=sua-chave-api
CLICKUP_API_KEY=pk_...
CLICKUP_LIST_ID=...
CLICKUP_CHANNEL_ID=...
GOOGLE_SHEET_ID=...
```

### 4. Importando Contas
Para importar as contas do arquivo `contas.csv` para o banco de dados Supabase:
```powershell
python src/import_csv.py
```

### 5. Executando o Scraper (Batch)
Para processar todas as contas ativas em lote:
```powershell
python src/batch_runner.py
```

---

## üìÇ Organiza√ß√£o dos Arquivos

| Arquivo | Descri√ß√£o |
| :--- | :--- |
| `src/scraper.py` | **Cora√ß√£o do projeto.** Cont√©m toda a l√≥gica de intera√ß√£o com a Livelo, detec√ß√£o de WAF (Access Denied), login e extra√ß√£o de pontos. |
| `src/batch_runner.py` | Orquestrador que percorre a lista de contas no Supabase, inicia o AdsPower e chama o scraper. |
| `src/adspower.py` | Controlador da API local do AdsPower (Start/Stop de perfis e captura de nomes). |
| `src/sheets_logger.py` | Respons√°vel por registrar o resultado de cada conta em tempo real no Google Sheets. |
| `src/clickup.py` | Envia notifica√ß√µes e cria tarefas no ClickUp quando h√° diverg√™ncia de saldo ou erros fatais. |
| `src/import_csv.py` | Script utilit√°rio para levar os dados do CSV para o Supabase. |
| `src/main.py` | (Opcional) Ponto de entrada para rodar como uma API FastAPI. |
| `requirements.txt` | Lista de bibliotecas Python necess√°rias. |
| `.gitignore` | Protege arquivos sens√≠veis (`.env`, `service_account.json`) de irem para o Git. |

---

## üõ°Ô∏è Defesas Implementadas
O scraper conta com l√≥gicas avan√ßadas de resili√™ncia:
*   **Parallel Blind Freeze**: Congela todas as abas abertas em paralelo para evitar que abas pesadas travem a automa√ß√£o.
*   **WAF Detection (Whitelist/Blacklist)**: Detecta bloqueios "Access Denied" da Akamai sem confundir com a Home Page leg√≠tima.
*   **Auto-Recovery (Retry)**: Se for bloqueado ap√≥s o login, ele fecha a aba, limpa o estado e tenta novamente em uma guia virgem.
*   **Login Guard**: Valida se o perfil do usu√°rio realmente carregou antes de tentar ler os pontos, evitando "Haliucina√ß√µes" de extra√ß√£o.
